{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Libraries\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.distributions import Categorical\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Traing on GPU if available\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to create episodes as namedtuple\n",
    "make_episode = namedtuple('Episode', \n",
    "                          field_names=['states', \n",
    "                                       'actions', \n",
    "                                       'rewards', \n",
    "                                       'init_command', \n",
    "                                       'total_return', \n",
    "                                       'length', \n",
    "                                       ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Utility Functions\n",
    "def initialize_replay_buffer(replay_size, n_episodes, last_few):\n",
    "    '''\n",
    "    Initialize replay buffer with warm-up episodes using random actions.\n",
    "    See section 2.3.1\n",
    "    \n",
    "    Params:\n",
    "        replay_size (int)\n",
    "        n_episodes (int)\n",
    "        last_few (int)\n",
    "    \n",
    "    Returns:\n",
    "        ReplayBuffer instance\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # This policy will generate random actions. Won't need state nor command\n",
    "    random_policy = lambda state, command: np.random.randint(env.action_space.n)\n",
    "    \n",
    "    buffer = ReplayBuffer(replay_size)\n",
    "    \n",
    "    for i in range(n_episodes):\n",
    "        command = sample_command(buffer, last_few)\n",
    "        episode = generate_episode(env, random_policy, command) # See Algorithm 2\n",
    "        buffer.add(episode)\n",
    "    \n",
    "    buffer.sort()\n",
    "    return buffer\n",
    "\n",
    "def initialize_behavior_function(state_size, \n",
    "                                 action_size, \n",
    "                                 hidden_size, \n",
    "                                 learning_rate, \n",
    "                                 command_scale):\n",
    "    '''\n",
    "    Initialize the behaviour function. See section 2.3.2\n",
    "    \n",
    "    Params:\n",
    "        state_size (int)\n",
    "        action_size (int)\n",
    "        hidden_size (int) -- NOTE: not used at the moment\n",
    "        learning_rate (float)\n",
    "        command_scale (List of float)\n",
    "    \n",
    "    Returns:\n",
    "        Behavior instance\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    behavior = Behavior(state_size, \n",
    "                        action_size, \n",
    "                        hidden_size, \n",
    "                        command_scale)\n",
    "    \n",
    "    behavior.init_optimizer(lr=learning_rate)\n",
    "    \n",
    "    return behavior\n",
    "\n",
    "def generate_episodes(env, behavior, buffer, n_episodes, last_few):\n",
    "    '''\n",
    "    1. Sample exploratory commands based on replay buffer\n",
    "    2. Generate episodes using Algorithm 2 and add to replay buffer\n",
    "    \n",
    "    Params:\n",
    "        env (OpenAI Gym Environment)\n",
    "        behavior (Behavior)\n",
    "        buffer (ReplayBuffer)\n",
    "        n_episodes (int)\n",
    "        last_few (int):\n",
    "            how many episodes we use to calculate the desired return and horizon\n",
    "    '''\n",
    "    \n",
    "    stochastic_policy = lambda state, command: behavior.action(state, command)\n",
    "    \n",
    "    for i in range(n_episodes_per_iter):\n",
    "        command = sample_command(buffer, last_few)\n",
    "        episode = generate_episode(env, stochastic_policy, command) # See Algorithm 2\n",
    "        buffer.add(episode)\n",
    "    \n",
    "    # Let's keep this buffer sorted\n",
    "    buffer.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Main Training Loop\n",
    "\n",
    "def UDRL(env, buffer=None, behavior=None, learning_history=[]):\n",
    "    '''\n",
    "    Upside-Down Reinforcement Learning main algrithm\n",
    "    \n",
    "    Params:\n",
    "        env (OpenAI Gym Environment)\n",
    "        buffer (ReplayBuffer):\n",
    "            if not passed in, new buffer is created\n",
    "        behavior (Behavior):\n",
    "            if not passed in, new behavior is created\n",
    "        learning_history (List of dict) -- default []\n",
    "    '''\n",
    "    \n",
    "    if buffer is None:\n",
    "        buffer = initialize_replay_buffer(replay_size, \n",
    "                                          n_warm_up_episodes, \n",
    "                                          last_few)\n",
    "    \n",
    "    if behavior is None:\n",
    "        behavior = initialize_behavior_function(state_size, \n",
    "                                                action_size, \n",
    "                                                hidden_size, \n",
    "                                                learning_rate, \n",
    "                                                [return_scale, horizon_scale])\n",
    "    \n",
    "    for i in range(1, n_main_iter+1):\n",
    "        mean_loss = train_behavior(behavior, buffer, n_updates_per_iter, batch_size)\n",
    "        \n",
    "        print('Iter: {}, Loss: {:.4f}'.format(i, mean_loss), end='\\r')\n",
    "        \n",
    "        # Sample exploratory commands and generate episodes\n",
    "        generate_episodes(env, \n",
    "                          behavior, \n",
    "                          buffer, \n",
    "                          n_episodes_per_iter,\n",
    "                          last_few)\n",
    "        \n",
    "        if i % evaluate_every == 0:\n",
    "            command = sample_command(buffer, last_few)\n",
    "            mean_return = evaluate_agent(env, behavior, command)\n",
    "            \n",
    "            learning_history.append({\n",
    "                'training_loss': mean_loss,\n",
    "                'desired_return': command[0],\n",
    "                'desired_horizon': command[1],\n",
    "                'actual_return': mean_return,\n",
    "            })\n",
    "            \n",
    "            if stop_on_solved and mean_return >= target_return: \n",
    "                break\n",
    "    \n",
    "    return behavior, buffer, learning_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Replay Buffer\n",
    "\n",
    "class ReplayBuffer():\n",
    "    '''\n",
    "    Replay buffer containing a fixed maximun number of trajectories with \n",
    "    the highest returns seen so far\n",
    "    \n",
    "    Params:\n",
    "        size (int)\n",
    "    \n",
    "    Attrs:\n",
    "        size (int)\n",
    "        buffer (List of episodes)\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, size=0):\n",
    "        self.size = size\n",
    "        self.buffer = []\n",
    "        \n",
    "    def add(self, episode):\n",
    "        '''\n",
    "        Params:\n",
    "            episode (namedtuple):\n",
    "                (states, actions, rewards, init_command, total_return, length)\n",
    "        '''\n",
    "        \n",
    "        self.buffer.append(episode)\n",
    "    \n",
    "    def get(self, num):\n",
    "        '''\n",
    "        Params:\n",
    "            num (int):\n",
    "                get the last `num` episodes from the buffer\n",
    "        '''\n",
    "        \n",
    "        return self.buffer[-num:]\n",
    "    \n",
    "    def random_batch(self, batch_size):\n",
    "        '''\n",
    "        Params:\n",
    "            batch_size (int)\n",
    "        \n",
    "        Returns:\n",
    "            Random batch of episodes from the buffer\n",
    "        '''\n",
    "        \n",
    "        idxs = np.random.randint(0, len(self), batch_size)\n",
    "        return [self.buffer[idx] for idx in idxs]\n",
    "    \n",
    "    def sort(self):\n",
    "        '''Keep the buffer sorted in ascending order by total return'''\n",
    "        \n",
    "        key_sort = lambda episode: episode.total_return\n",
    "        self.buffer = sorted(self.buffer, key=key_sort)[-self.size:]\n",
    "    \n",
    "    def save(self, filename):\n",
    "        '''Save the buffer in numpy format\n",
    "        \n",
    "        Param:\n",
    "            filename (str)\n",
    "        '''\n",
    "        \n",
    "        np.save(filename, self.buffer)\n",
    "    \n",
    "    def load(self, filename):\n",
    "        '''Load a numpy format file\n",
    "        \n",
    "        Params:\n",
    "            filename (str)\n",
    "        '''\n",
    "        \n",
    "        raw_buffer = np.load(filename)\n",
    "        self.size = len(raw_buffer)\n",
    "        self.buffer = \\\n",
    "            [make_episode(episode[0], episode[1], episode[2], episode[3], episode[4], episode[5]) \\\n",
    "             for episode in raw_buffer]\n",
    "    \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            Size of the buffer\n",
    "        '''\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Behavior Function\n",
    "\n",
    "class Behavior(nn.Module):\n",
    "    '''\n",
    "    Behavour function that produces actions based on a state and command.\n",
    "    NOTE: At the moment I'm fixing the amount of units and layers.\n",
    "    TODO: Make hidden layers configurable.\n",
    "    \n",
    "    Params:\n",
    "        state_size (int)\n",
    "        action_size (int)\n",
    "        hidden_size (int) -- NOTE: not used at the moment\n",
    "        command_scale (List of float)\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, \n",
    "                 state_size, \n",
    "                 action_size, \n",
    "                 hidden_size, \n",
    "                 command_scale = [1, 1]):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.command_scale = torch.FloatTensor(command_scale).to(device)\n",
    "        \n",
    "        self.state_fc = nn.Sequential(nn.Linear(state_size, 64), \n",
    "                                      nn.Tanh())\n",
    "        \n",
    "        self.command_fc = nn.Sequential(nn.Linear(2, 64), \n",
    "                                        nn.Sigmoid())\n",
    "        \n",
    "        self.output_fc = nn.Sequential(nn.Linear(64, 128), \n",
    "                                       nn.ReLU(), \n",
    "#                                        nn.Dropout(0.2),\n",
    "                                       nn.Linear(128, 128), \n",
    "                                       nn.ReLU(), \n",
    "#                                        nn.Dropout(0.2),\n",
    "                                       nn.Linear(128, 128), \n",
    "                                       nn.ReLU(), \n",
    "                                       nn.Linear(128, action_size))\n",
    "        \n",
    "        self.to(device)\n",
    "        \n",
    "    \n",
    "    def forward(self, state, command):\n",
    "        '''Forward pass\n",
    "        \n",
    "        Params:\n",
    "            state (List of float)\n",
    "            command (List of float)\n",
    "        \n",
    "        Returns:\n",
    "            FloatTensor -- action logits\n",
    "        '''\n",
    "        \n",
    "        state_output = self.state_fc(state)\n",
    "        command_output = self.command_fc(command * self.command_scale)\n",
    "        embedding = torch.mul(state_output, command_output)\n",
    "        return self.output_fc(embedding)\n",
    "    \n",
    "    def action(self, state, command):\n",
    "        '''\n",
    "        Params:\n",
    "            state (List of float)\n",
    "            command (List of float)\n",
    "            \n",
    "        Returns:\n",
    "            int -- stochastic action\n",
    "        '''\n",
    "        \n",
    "        logits = self.forward(state, command)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        dist = Categorical(probs)\n",
    "        return dist.sample().item()\n",
    "    \n",
    "    def greedy_action(self, state, command):\n",
    "        '''\n",
    "        Params:\n",
    "            state (List of float)\n",
    "            command (List of float)\n",
    "            \n",
    "        Returns:\n",
    "            int -- greedy action\n",
    "        '''\n",
    "        \n",
    "        logits = self.forward(state, command)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        return np.argmax(probs.detach().cpu().numpy())\n",
    "    \n",
    "    def init_optimizer(self, optim=Adam, lr=0.003):\n",
    "        '''Initialize GD optimizer\n",
    "        \n",
    "        Params:\n",
    "            optim (Optimizer) -- default Adam\n",
    "            lr (float) -- default 0.003\n",
    "        '''\n",
    "        \n",
    "        self.optim = optim(self.parameters(), lr=lr)\n",
    "    \n",
    "    def save(self, filename):\n",
    "        '''Save the model's parameters\n",
    "        Param:\n",
    "            filename (str)\n",
    "        '''\n",
    "        \n",
    "        torch.save(self.state_dict(), filename)\n",
    "    \n",
    "    def load(self, filename):\n",
    "        '''Load the model's parameters\n",
    "        \n",
    "        Params:\n",
    "            filename (str)\n",
    "        '''\n",
    "        \n",
    "        self.load_state_dict(torch.load(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate Episode\n",
    "\n",
    "def generate_episode(env, policy, init_command=[1, 1]):\n",
    "    '''\n",
    "    Generate an episode using the Behaviour function.\n",
    "    \n",
    "    Params:\n",
    "        env (OpenAI Gym Environment)\n",
    "        policy (func)\n",
    "        init_command (List of float) -- default [1, 1]\n",
    "    \n",
    "    Returns:\n",
    "        Namedtuple (states, actions, rewards, init_command, total_return, length)\n",
    "    '''\n",
    "    \n",
    "    command = init_command.copy()\n",
    "    desired_return = command[0]\n",
    "    desired_horizon = command[1]\n",
    "    \n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    \n",
    "    time_steps = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    state = env.reset().tolist()\n",
    "    \n",
    "    while not done:\n",
    "        state_input = torch.FloatTensor(state).to(device)\n",
    "        command_input = torch.FloatTensor(command).to(device)\n",
    "        action = policy(state_input, command_input)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Modifying a bit the reward function punishing the agent, -100, \n",
    "        # if it reaches hyperparam max_steps. The reason I'm doing this \n",
    "        # is because I noticed that the agent tends to gather points by \n",
    "        # landing the spaceshipt and getting out and back in the landing \n",
    "        # area over and over again, never switching off the engines. \n",
    "        # The longer it does that the more reward it gathers. Later on in \n",
    "        # the training it realizes that it can get more points by turning \n",
    "        # off the engines, but takes more epochs to get to that conclusion.\n",
    "        if not done and time_steps > max_steps:\n",
    "            done = True\n",
    "            reward = max_steps_reward\n",
    "        \n",
    "        # Sparse rewards. Cumulative reward is delayed until the end of each episode\n",
    "#         total_rewards += reward\n",
    "#         reward = total_rewards if done else 0.0\n",
    "        \n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        state = next_state.tolist()\n",
    "        \n",
    "        # Clipped such that it's upper-bounded by the maximum return achievable in the env\n",
    "        desired_return = min(desired_return - reward, max_reward)\n",
    "        \n",
    "        # Make sure it's always a valid horizon\n",
    "        desired_horizon = max(desired_horizon - 1, 1)\n",
    "    \n",
    "        command = [desired_return, desired_horizon]\n",
    "        time_steps += 1\n",
    "        \n",
    "    return make_episode(states, actions, rewards, init_command, sum(rewards), time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train Behavior Function\n",
    "def train_behavior(behavior, buffer, n_updates, batch_size):\n",
    "    '''Training loop\n",
    "    \n",
    "    Params:\n",
    "        behavior (Behavior)\n",
    "        buffer (ReplayBuffer)\n",
    "        n_updates (int):\n",
    "            how many updates we're gonna perform\n",
    "        batch_size (int):\n",
    "            size of the bacth we're gonna use to train on\n",
    "    \n",
    "    Returns:\n",
    "        float -- mean loss after all the updates\n",
    "    '''\n",
    "    all_loss = []\n",
    "    for update in range(n_updates):\n",
    "        episodes = buffer.random_batch(batch_size)\n",
    "        \n",
    "        batch_states = []\n",
    "        batch_commands = []\n",
    "        batch_actions = []\n",
    "        \n",
    "        for episode in episodes:\n",
    "            T = episode.length\n",
    "            t1 = np.random.randint(0, T)\n",
    "            t2 = np.random.randint(t1+1, T+1)\n",
    "            dr = sum(episode.rewards[t1:t2])\n",
    "            dh = t2 - t1\n",
    "            \n",
    "            st1 = episode.states[t1]\n",
    "            at1 = episode.actions[t1]\n",
    "            \n",
    "            batch_states.append(st1)\n",
    "            batch_actions.append(at1)\n",
    "            batch_commands.append([dr, dh])\n",
    "        \n",
    "        batch_states = torch.FloatTensor(batch_states).to(device)\n",
    "        batch_commands = torch.FloatTensor(batch_commands).to(device)\n",
    "        batch_actions = torch.LongTensor(batch_actions).to(device)\n",
    "        \n",
    "        pred = behavior(batch_states, batch_commands)\n",
    "        \n",
    "        loss = F.cross_entropy(pred, batch_actions)\n",
    "        \n",
    "        behavior.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        behavior.optim.step()\n",
    "        \n",
    "        all_loss.append(loss.item())\n",
    "    \n",
    "    return np.mean(all_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sample Exploratory Commands\n",
    "def sample_command(buffer, last_few):\n",
    "    '''Sample a exploratory command\n",
    "    \n",
    "    Params:\n",
    "        buffer (ReplayBuffer)\n",
    "        last_few:\n",
    "            how many episodes we're gonna look at to calculate \n",
    "            the desired return and horizon.\n",
    "    \n",
    "    Returns:\n",
    "        List of float -- command\n",
    "    '''\n",
    "    if len(buffer) == 0: return [1, 1]\n",
    "    \n",
    "    # 1.\n",
    "    commands = buffer.get(last_few)\n",
    "    \n",
    "    # 2.\n",
    "    lengths = [command.length for command in commands]\n",
    "    desired_horizon = round(np.mean(lengths))\n",
    "    \n",
    "    # 3.\n",
    "    returns = [command.total_return for command in commands]\n",
    "    mean_return, std_return = np.mean(returns), np.std(returns)\n",
    "    desired_return = np.random.uniform(mean_return, mean_return+std_return)\n",
    "    \n",
    "    return [desired_return, desired_horizon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluation\n",
    "def evaluate_agent(env, behavior, command, render=False):\n",
    "    '''\n",
    "    Evaluate the agent performance by running an episode\n",
    "    following Algorithm 2 steps\n",
    "    \n",
    "    Params:\n",
    "        env (OpenAI Gym Environment)\n",
    "        behavior (Behavior)\n",
    "        command (List of float)\n",
    "        render (bool) -- default False:\n",
    "            will render the environment to visualize the agent performance\n",
    "    '''\n",
    "    behavior.eval()\n",
    "    \n",
    "    print('\\nEvaluation.', end=' ')\n",
    "        \n",
    "    desired_return = command[0]\n",
    "    desired_horizon = command[1]\n",
    "    \n",
    "    print('Desired return: {:.2f}, Desired horizon: {:.2f}.'.format(desired_return, desired_horizon), end=' ')\n",
    "    \n",
    "    all_rewards = []\n",
    "    \n",
    "    for e in range(n_evals):\n",
    "        \n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        state = env.reset().tolist()\n",
    "    \n",
    "        while not done:\n",
    "            if render: env.render()\n",
    "            \n",
    "            state_input = torch.FloatTensor(state).to(device)\n",
    "            command_input = torch.FloatTensor(command).to(device)\n",
    "\n",
    "            action = behavior.greedy_action(state_input, command_input)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            total_reward += reward\n",
    "            state = next_state.tolist()\n",
    "\n",
    "            desired_return = min(desired_return - reward, max_reward)\n",
    "            desired_horizon = max(desired_horizon - 1, 1)\n",
    "\n",
    "            command = [desired_return, desired_horizon]\n",
    "        \n",
    "        if render: env.close()\n",
    "        \n",
    "        all_rewards.append(total_reward)\n",
    "    \n",
    "    mean_return = np.mean(all_rewards)\n",
    "    print('Reward achieved: {:.2f}'.format(mean_return))\n",
    "    \n",
    "    behavior.train()\n",
    "    \n",
    "    return mean_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ast (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ast (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pip in /opt/anaconda3/lib/python3.7/site-packages (22.0.4)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ast (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ast (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ast (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!/opt/anaconda3/bin/python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ast (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ast (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: gym in /opt/anaconda3/lib/python3.7/site-packages (0.17.3)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /opt/anaconda3/lib/python3.7/site-packages (from gym) (1.2.0)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /opt/anaconda3/lib/python3.7/site-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /opt/anaconda3/lib/python3.7/site-packages (from gym) (1.19.5)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.7/site-packages (from gym) (1.4.1)\n",
      "Requirement already satisfied: future in /opt/anaconda3/lib/python3.7/site-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.18.2)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ast (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ast (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ast (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'Rocket_Lander_Gym' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Jeetu95/Rocket_Lander_Gym.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mRocket_Lander_Gym\u001b[m\u001b[m    Upside Down RL.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ast (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[31mERROR: Directory '.' is not installable. Neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ast (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ast (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!cd Rocket_Lander_Gym/\n",
    "!pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ast (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ast (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: ribs[all] in /opt/anaconda3/lib/python3.7/site-packages (0.4.0)\n",
      "Requirement already satisfied: gym~=0.17.0 in /opt/anaconda3/lib/python3.7/site-packages (0.17.3)\n",
      "Requirement already satisfied: Box2D~=2.3.10 in /opt/anaconda3/lib/python3.7/site-packages (2.3.10)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.7/site-packages (4.55.1)\n",
      "Requirement already satisfied: decorator>=4.0.0 in /opt/anaconda3/lib/python3.7/site-packages (from ribs[all]) (4.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.7/site-packages (from ribs[all]) (3.1.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /opt/anaconda3/lib/python3.7/site-packages (from ribs[all]) (0.22.1)\n",
      "Requirement already satisfied: toml>=0.10.0 in /opt/anaconda3/lib/python3.7/site-packages (from ribs[all]) (0.10.2)\n",
      "Requirement already satisfied: pandas>=1.0.0 in /opt/anaconda3/lib/python3.7/site-packages (from ribs[all]) (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /opt/anaconda3/lib/python3.7/site-packages (from ribs[all]) (1.19.5)\n",
      "Requirement already satisfied: numba>=0.51.0 in /opt/anaconda3/lib/python3.7/site-packages (from ribs[all]) (0.55.1)\n",
      "Requirement already satisfied: scipy>=1.4.0 in /opt/anaconda3/lib/python3.7/site-packages (from ribs[all]) (1.4.1)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.0 in /opt/anaconda3/lib/python3.7/site-packages (from ribs[all]) (2.4.0)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in /opt/anaconda3/lib/python3.7/site-packages (from ribs[all]) (3.1.1)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /opt/anaconda3/lib/python3.7/site-packages (from gym~=0.17.0) (1.2.0)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /opt/anaconda3/lib/python3.7/site-packages (from gym~=0.17.0) (1.5.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/anaconda3/lib/python3.7/site-packages (from matplotlib>=3.0.0->ribs[all]) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.7/site-packages (from matplotlib>=3.0.0->ribs[all]) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/lib/python3.7/site-packages (from matplotlib>=3.0.0->ribs[all]) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/anaconda3/lib/python3.7/site-packages (from matplotlib>=3.0.0->ribs[all]) (2.8.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.7/site-packages (from numba>=0.51.0->ribs[all]) (51.0.0.post20201207)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in /opt/anaconda3/lib/python3.7/site-packages (from numba>=0.51.0->ribs[all]) (0.38.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/anaconda3/lib/python3.7/site-packages (from pandas>=1.0.0->ribs[all]) (2020.5)\n",
      "Requirement already satisfied: future in /opt/anaconda3/lib/python3.7/site-packages (from pyglet<=1.5.0,>=1.4.0->gym~=0.17.0) (0.18.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/anaconda3/lib/python3.7/site-packages (from scikit-learn>=0.20.0->ribs[all]) (1.0.0)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.7/site-packages (from cycler>=0.10->matplotlib>=3.0.0->ribs[all]) (1.15.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ast (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ast (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ast (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ast (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ast (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: box2d-py in /opt/anaconda3/lib/python3.7/site-packages (2.3.8)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ast (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ast (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ast (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ast (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ast (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: box2d-py in /opt/anaconda3/lib/python3.7/site-packages (2.3.8)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ast (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ast (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ast (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ast (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ast (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: gym[Box_2D] in /opt/anaconda3/lib/python3.7/site-packages (0.17.3)\n",
      "\u001b[33mWARNING: gym 0.17.3 does not provide the extra 'box_2d'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /opt/anaconda3/lib/python3.7/site-packages (from gym[Box_2D]) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /opt/anaconda3/lib/python3.7/site-packages (from gym[Box_2D]) (1.19.5)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.7/site-packages (from gym[Box_2D]) (1.4.1)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /opt/anaconda3/lib/python3.7/site-packages (from gym[Box_2D]) (1.5.0)\n",
      "Requirement already satisfied: future in /opt/anaconda3/lib/python3.7/site-packages (from pyglet<=1.5.0,>=1.4.0->gym[Box_2D]) (0.18.2)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ast (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ast (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ast (/opt/anaconda3/lib/python3.7/site-packages)\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%pip install ribs[all] gym~=0.17.0 Box2D~=2.3.10 tqdm\n",
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "!conda install swig # needed to build Box2D in the pip install\n",
    "!pip install box2d-py # a repackaged version of pybox2d\n",
    "!pip3 install box2d-py\n",
    "!pip3 install gym[Box_2D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run Experiments\n",
    "import gym\n",
    "#import Rocket_Lander_Gym\n",
    "import gym.spaces\n",
    "#import rocket_lander_gym\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\") # RocketLander-v0 | LunarLander-v2 | MountainCar-v0 | CartPole-v0\n",
    "_ = env.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size: 8\n",
      "Action size: 4\n"
     ]
    }
   ],
   "source": [
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "print('State size: {}'.format(state_size))\n",
    "print('Action size: {}'.format(action_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparamters\n",
    "\n",
    "# Number of iterations in the main loop\n",
    "n_main_iter = 700\n",
    "\n",
    "# Number of (input, target) pairs per batch used for training the behavior function\n",
    "batch_size = 768\n",
    "\n",
    "# Scaling factor for desired horizon input\n",
    "horizon_scale = 0.01\n",
    "\n",
    "# Number of episodes from the end of the replay buffer used for sampling exploratory\n",
    "# commands\n",
    "last_few = 75\n",
    "\n",
    "# Learning rate for the ADAM optimizer\n",
    "learning_rate = 0.0003\n",
    "\n",
    "# Number of exploratory episodes generated per step of UDRL training\n",
    "n_episodes_per_iter = 20\n",
    "\n",
    "# Number of gradient-based updates of the behavior function per step of UDRL training\n",
    "n_updates_per_iter = 100\n",
    "\n",
    "# Number of warm up episodes at the beginning of training\n",
    "n_warm_up_episodes = 10\n",
    "\n",
    "# Maximum size of the replay buffer (in episodes)\n",
    "replay_size = 500\n",
    "\n",
    "# Scaling factor for desired return input\n",
    "return_scale = 0.02\n",
    "\n",
    "# Evaluate the agent after `evaluate_every` iterations\n",
    "evaluate_every = 10\n",
    "\n",
    "# Target return before breaking out of the training loop\n",
    "target_return = 200\n",
    "\n",
    "# Maximun reward given by the environment\n",
    "max_reward = 250\n",
    "\n",
    "# Maximun steps allowed\n",
    "max_steps = 300\n",
    "\n",
    "# Reward after reaching `max_steps` (punishment, hence negative reward)\n",
    "max_steps_reward = -50\n",
    "\n",
    "# Hidden units\n",
    "hidden_size = 32\n",
    "\n",
    "# Times we evaluate the agent\n",
    "n_evals = 1\n",
    "\n",
    "# Will stop the training when the agent gets `target_return` `n_evals` times\n",
    "stop_on_solved = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 10, Loss: 1.3519\n",
      "Evaluation. Desired return: -45.32, Desired horizon: 82.00. Reward achieved: -150.47\n",
      "Iter: 20, Loss: 1.3371\n",
      "Evaluation. Desired return: -43.47, Desired horizon: 90.00. Reward achieved: -143.40\n",
      "Iter: 30, Loss: 1.3341\n",
      "Evaluation. Desired return: -21.94, Desired horizon: 93.00. Reward achieved: -118.82\n",
      "Iter: 40, Loss: 1.3314\n",
      "Evaluation. Desired return: -6.44, Desired horizon: 93.00. Reward achieved: -137.99\n",
      "Iter: 50, Loss: 1.3247\n",
      "Evaluation. Desired return: -26.18, Desired horizon: 94.00. Reward achieved: -123.75\n",
      "Iter: 60, Loss: 1.3195\n",
      "Evaluation. Desired return: -12.47, Desired horizon: 99.00. Reward achieved: -111.98\n",
      "Iter: 70, Loss: 1.3130\n",
      "Evaluation. Desired return: 7.00, Desired horizon: 99.00. Reward achieved: -150.01\n",
      "Iter: 80, Loss: 1.3110\n",
      "Evaluation. Desired return: -16.16, Desired horizon: 105.00. Reward achieved: -237.73\n",
      "Iter: 90, Loss: 1.3040\n",
      "Evaluation. Desired return: 16.33, Desired horizon: 105.00. Reward achieved: -198.64\n",
      "Iter: 100, Loss: 1.2921\n",
      "Evaluation. Desired return: -7.38, Desired horizon: 104.00. Reward achieved: -495.02\n",
      "Iter: 110, Loss: 1.2844\n",
      "Evaluation. Desired return: 2.32, Desired horizon: 108.00. Reward achieved: -257.07\n",
      "Iter: 120, Loss: 1.2666\n",
      "Evaluation. Desired return: 9.24, Desired horizon: 112.00. Reward achieved: -45.89\n",
      "Iter: 130, Loss: 1.2475\n",
      "Evaluation. Desired return: 19.07, Desired horizon: 121.00. Reward achieved: -125.04\n",
      "Iter: 140, Loss: 1.2348\n",
      "Evaluation. Desired return: 34.84, Desired horizon: 129.00. Reward achieved: -102.67\n",
      "Iter: 150, Loss: 1.2103\n",
      "Evaluation. Desired return: 30.13, Desired horizon: 133.00. Reward achieved: -134.51\n",
      "Iter: 160, Loss: 1.1973\n",
      "Evaluation. Desired return: 50.51, Desired horizon: 151.00. Reward achieved: -64.03\n",
      "Iter: 170, Loss: 1.1778\n",
      "Evaluation. Desired return: 45.67, Desired horizon: 170.00. Reward achieved: -96.63\n",
      "Iter: 180, Loss: 1.1569\n",
      "Evaluation. Desired return: 46.73, Desired horizon: 188.00. Reward achieved: -79.10\n",
      "Iter: 190, Loss: 1.1458\n",
      "Evaluation. Desired return: 63.82, Desired horizon: 232.00. Reward achieved: -47.54\n",
      "Iter: 200, Loss: 1.1295\n",
      "Evaluation. Desired return: 95.36, Desired horizon: 281.00. Reward achieved: -104.00\n",
      "Iter: 210, Loss: 1.1089\n",
      "Evaluation. Desired return: 97.83, Desired horizon: 300.00. Reward achieved: -69.81\n",
      "Iter: 220, Loss: 1.0920\n",
      "Evaluation. Desired return: 116.37, Desired horizon: 301.00. Reward achieved: -150.25\n",
      "Iter: 230, Loss: 1.0859\n",
      "Evaluation. Desired return: 106.66, Desired horizon: 302.00. Reward achieved: -140.21\n",
      "Iter: 240, Loss: 1.0695\n",
      "Evaluation. Desired return: 109.74, Desired horizon: 302.00. Reward achieved: -71.24\n",
      "Iter: 250, Loss: 1.0634\n",
      "Evaluation. Desired return: 124.84, Desired horizon: 302.00. Reward achieved: -284.91\n",
      "Iter: 260, Loss: 1.0524\n",
      "Evaluation. Desired return: 127.33, Desired horizon: 302.00. Reward achieved: -127.19\n",
      "Iter: 270, Loss: 1.0467\n",
      "Evaluation. Desired return: 120.74, Desired horizon: 302.00. Reward achieved: 4.26\n",
      "Iter: 280, Loss: 1.0400\n",
      "Evaluation. Desired return: 136.34, Desired horizon: 302.00. Reward achieved: 178.19\n",
      "Iter: 290, Loss: 1.0362\n",
      "Evaluation. Desired return: 130.89, Desired horizon: 302.00. Reward achieved: -6.87\n",
      "Iter: 300, Loss: 1.0349\n",
      "Evaluation. Desired return: 127.92, Desired horizon: 302.00. Reward achieved: 206.38\n",
      "Iter: 310, Loss: 1.0370\n",
      "Evaluation. Desired return: 136.97, Desired horizon: 302.00. Reward achieved: 17.30\n",
      "Iter: 320, Loss: 1.0311\n",
      "Evaluation. Desired return: 132.89, Desired horizon: 302.00. Reward achieved: -70.68\n",
      "Iter: 330, Loss: 1.0279\n",
      "Evaluation. Desired return: 148.76, Desired horizon: 302.00. Reward achieved: 212.25\n",
      "Iter: 340, Loss: 1.0265\n",
      "Evaluation. Desired return: 151.77, Desired horizon: 302.00. Reward achieved: 221.36\n",
      "Iter: 350, Loss: 1.0258\n",
      "Evaluation. Desired return: 149.16, Desired horizon: 302.00. Reward achieved: 206.02\n",
      "Iter: 360, Loss: 1.0300\n",
      "Evaluation. Desired return: 136.96, Desired horizon: 301.00. Reward achieved: 203.92\n",
      "Iter: 370, Loss: 1.0261\n",
      "Evaluation. Desired return: 140.16, Desired horizon: 301.00. Reward achieved: 201.89\n",
      "Iter: 380, Loss: 1.0252\n",
      "Evaluation. Desired return: 156.58, Desired horizon: 301.00. Reward achieved: 225.55\n",
      "Iter: 390, Loss: 1.0227\n",
      "Evaluation. Desired return: 168.43, Desired horizon: 301.00. Reward achieved: 242.80\n",
      "Iter: 400, Loss: 1.0208\n",
      "Evaluation. Desired return: 155.71, Desired horizon: 301.00. Reward achieved: 251.77\n",
      "Iter: 410, Loss: 1.0151\n",
      "Evaluation. Desired return: 165.25, Desired horizon: 301.00. Reward achieved: 188.00\n",
      "Iter: 420, Loss: 1.0163\n",
      "Evaluation. Desired return: 151.75, Desired horizon: 300.00. Reward achieved: 200.13\n",
      "Iter: 430, Loss: 1.0100\n",
      "Evaluation. Desired return: 179.29, Desired horizon: 300.00. Reward achieved: 242.92\n",
      "Iter: 440, Loss: 1.0117\n",
      "Evaluation. Desired return: 149.19, Desired horizon: 300.00. Reward achieved: 197.39\n",
      "Iter: 450, Loss: 1.0047\n",
      "Evaluation. Desired return: 164.78, Desired horizon: 300.00. Reward achieved: 242.33\n",
      "Iter: 460, Loss: 1.0057\n",
      "Evaluation. Desired return: 190.73, Desired horizon: 299.00. Reward achieved: 268.54\n",
      "Iter: 470, Loss: 0.9967\n",
      "Evaluation. Desired return: 215.50, Desired horizon: 296.00. Reward achieved: 284.18\n",
      "Iter: 480, Loss: 0.9814\n",
      "Evaluation. Desired return: 271.21, Desired horizon: 273.00. Reward achieved: 227.51\n",
      "Iter: 490, Loss: 0.9269\n",
      "Evaluation. Desired return: 289.74, Desired horizon: 263.00. Reward achieved: 268.53\n",
      "Iter: 500, Loss: 0.8828\n",
      "Evaluation. Desired return: 294.67, Desired horizon: 258.00. Reward achieved: 274.60\n",
      "Iter: 510, Loss: 0.8274\n",
      "Evaluation. Desired return: 302.48, Desired horizon: 262.00. Reward achieved: 271.16\n",
      "Iter: 520, Loss: 0.8069\n",
      "Evaluation. Desired return: 302.73, Desired horizon: 262.00. Reward achieved: 250.67\n",
      "Iter: 530, Loss: 0.7955\n",
      "Evaluation. Desired return: 307.55, Desired horizon: 262.00. Reward achieved: 250.24\n",
      "Iter: 540, Loss: 0.7985\n",
      "Evaluation. Desired return: 309.47, Desired horizon: 266.00. Reward achieved: 286.43\n",
      "Iter: 550, Loss: 0.7889\n",
      "Evaluation. Desired return: 308.43, Desired horizon: 264.00. Reward achieved: 263.50\n",
      "Iter: 560, Loss: 0.7827\n",
      "Evaluation. Desired return: 309.48, Desired horizon: 263.00. Reward achieved: 285.17\n",
      "Iter: 570, Loss: 0.7816\n",
      "Evaluation. Desired return: 311.25, Desired horizon: 263.00. Reward achieved: 251.99\n",
      "Iter: 580, Loss: 0.7785\n",
      "Evaluation. Desired return: 312.17, Desired horizon: 262.00. Reward achieved: 279.44\n",
      "Iter: 590, Loss: 0.7713\n",
      "Evaluation. Desired return: 313.33, Desired horizon: 263.00. Reward achieved: 272.70\n",
      "Iter: 600, Loss: 0.7696\n",
      "Evaluation. Desired return: 315.20, Desired horizon: 263.00. Reward achieved: 287.84\n",
      "Iter: 610, Loss: 0.7706\n",
      "Evaluation. Desired return: 314.80, Desired horizon: 263.00. Reward achieved: 293.25\n",
      "Iter: 620, Loss: 0.7662\n",
      "Evaluation. Desired return: 313.35, Desired horizon: 263.00. Reward achieved: 250.90\n",
      "Iter: 630, Loss: 0.7622\n",
      "Evaluation. Desired return: 313.15, Desired horizon: 263.00. Reward achieved: 293.20\n",
      "Iter: 640, Loss: 0.7584\n",
      "Evaluation. Desired return: 314.85, Desired horizon: 263.00. Reward achieved: 258.86\n",
      "Iter: 650, Loss: 0.7577\n",
      "Evaluation. Desired return: 316.20, Desired horizon: 262.00. Reward achieved: 242.17\n",
      "Iter: 660, Loss: 0.7538\n",
      "Evaluation. Desired return: 316.90, Desired horizon: 261.00. Reward achieved: 253.03\n",
      "Iter: 670, Loss: 0.7577\n",
      "Evaluation. Desired return: 315.82, Desired horizon: 261.00. Reward achieved: 238.78\n",
      "Iter: 680, Loss: 0.7553\n",
      "Evaluation. Desired return: 315.40, Desired horizon: 262.00. Reward achieved: 246.65\n",
      "Iter: 690, Loss: 0.7562\n",
      "Evaluation. Desired return: 317.20, Desired horizon: 262.00. Reward achieved: 242.60\n",
      "Iter: 700, Loss: 0.7553\n",
      "Evaluation. Desired return: 318.68, Desired horizon: 261.00. Reward achieved: 260.74\n"
     ]
    }
   ],
   "source": [
    "behavior, buffer, learning_history = UDRL(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation. Desired return: 250.00, Desired horizon: 230.00. Reward achieved: 218.08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "218.0821451108621"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_agent(env, behavior, [250, 230], render=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
